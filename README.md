# Non-Objectives/To-Do

- I have not used the skeleton you have provided to code the solution. 
- The csv file espcially for Company B contains data mis-aligned and also wrong values for gender columns, some other data-type based errors which is not fixed yet
- No test-cases are written and code is not commented to the extent I comment in production ready code. 
- The code is not production ready and it is only to show-case sample of coding style. 
- Data model is not efficient for reading and analytics, however, I didn't proceed to use the denormalized data as it is, to answer analytical questions using the computational framework I have chosen which is [Polars] (https://pola-rs.github.io/polars-book/user-guide/quickstart/intro.html)
- Code is not yet black formatted. 

# Solution Approach

- The problem given is like reverse modeling and reach operational data warehouse stage optimized for writing data. 
- I have normalized the data close to 3NF to make it efficient for writing.
- I have commented out the code to load the company B data because I am yet to create necessary boolean mask and filter corrupted values in certain columns.
- Please check below commands to run the code to either load the datasets and/or query the database. 
- The above said normalized reverse model is not efficient for analytics and reading lots of row oriented datasets, however, the simple question asked is answered in one of the possible ways. 

# Poetry Installation

- Install poetry by following instructions [here](https://python-poetry.org/docs/#installation)

```sh
sudo apt install python3-poetry
```

# Steps to Run

> Run all the below from the root of the project

- I have also checked in the poetry.lock file which helps you to run with right set of dependencies. You can either install dependencies as a first step and run or directly run the below commands. 
- Please go ahead and run following to install the dependencies as a first step. 

```sh
poetry install
```
- Poetry creates its own virtual environment to install the dependencies from the toml file, so you don't have to create your own virtual environemtn using conda or venv. You can read more [here](https://python-poetry.org/docs/basic-usage/#activating-the-virtual-environment)

- Load the exported data to embedded Sqlite as you prefer Sqlite
```sh
poetry run python syndio_sugiv_solution/load_exported_data.py
```

- Please run the following command to get answer to the simple question 'average employee compensation for each job group by gender'
- For any other advanced analytical queries, the above said reverse model is not efficient for reading. The problem seems to me like, we are re-creating the operational data-warehouse model from exported data and then also try to ask questions on top of that. 
- Initially, I was planning to take the exported data as it is and create the next level of layer on top of base layer(raw data) and use efficient computational frameworks like Polars (on top of Apache Arrow) to answer questions but since the instructions given are to use Sqlite and normalize, I went ahead and followed your instructions. 

```sh
poetry run python syndio_sugiv_solution/solution.py
```

- /data contains the final SQLite DB file called input_data.db
- load_exported_data.py contains logic to normalize and load the data to Sqlite.
- I have leveraged ORM Sqlalchemy and Python as my data modeling language for this exercise. In my other github projects check-in, you will see me also using cube.dev Javascript as a language to model data. 


- The following are the SQL generated by SQLalchemy based on the normalized model. 

```sql
sqlite> .schema employee
CREATE TABLE employee (
	employee_id INTEGER NOT NULL, 
	employee_job_group VARCHAR(50), 
	employee_job_level VARCHAR(100), 
	employee_job_title VARCHAR(100), 
	employee_gender VARCHAR(100), 
	employee_race VARCHAR(50), 
	employee_compensation FLOAT, 
	employee_years_of_experience INTEGER, 
	employee_time_in_role INTEGER, 
	employee_city VARCHAR(100), 
	employee_company VARCHAR(100), 
	PRIMARY KEY (employee_id), 
	FOREIGN KEY(employee_city) REFERENCES geo (employee_city), 
	FOREIGN KEY(employee_company) REFERENCES company (company_name)
);
```

```sql
CREATE TABLE company (
	company_name VARCHAR(100) NOT NULL, 
	company_headquarters_city VARCHAR(100), 
	company_headquarters_state VARCHAR(10), 
	company_sector VARCHAR(50), 
	PRIMARY KEY (company_name)
);
```
```sql
CREATE TABLE geo (
	geo_zone_id VARCHAR, 
	employee_geo_zone VARCHAR(100), 
	employee_city VARCHAR(100) NOT NULL, 
	employee_state VARCHAR(10), 
	geo_zone_cost_of_living_factor FLOAT, 
	PRIMARY KEY (employee_city)
);
```